{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2_Text_Generation_Tests.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxaVgbr8/YUwvDcD0tjH5K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/royn5618/Deep-Learning-Things/blob/main/NLP/HuggingFace_Transformers/GPT_2_Text_Generation_Tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSfBL0NFLEiC",
        "outputId": "c692198a-96ff-4ec8-e054-281c9922e421"
      },
      "source": [
        "!pip install transformers "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpF77MkzLGr4"
      },
      "source": [
        "import transformers\n",
        "import torch"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAYXv5yhLRVZ"
      },
      "source": [
        "gpt_tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2-large')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "854LjB8QLSHK"
      },
      "source": [
        "gpt_model = transformers.GPT2LMHeadModel.from_pretrained('gpt2-large')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL6depclL-JL"
      },
      "source": [
        "prompt_text = 'It was raining when I started walking'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1a6389JMEQG",
        "outputId": "14d931ba-8d57-4edd-b573-95074600dd84"
      },
      "source": [
        "encoded_prompt = gpt_tokenizer.encode(prompt_text, \n",
        "                                      add_special_tokens=False, \n",
        "                                      return_tensors=\"pt\")\n",
        "\n",
        "encoded_prompt "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1026,   373, 43079,   618,   314,  2067,  6155]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9rFZE0zMU5q",
        "outputId": "b80a0c5c-6410-46a9-c30b-5c663499ac64"
      },
      "source": [
        "## IGNORE\n",
        "encoded_prompt_test = gpt_tokenizer.encode(prompt_text, \n",
        "                                      add_special_tokens=True, \n",
        "                                      return_tensors=\"pt\")\n",
        "\n",
        "encoded_prompt_test"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1026,   373, 43079,   618,   314,  2067,  6155]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2aPyXztMZoe",
        "outputId": "879e7535-1fba-4c5b-c0c5-3b0308196cb4"
      },
      "source": [
        "## IGNORE\n",
        "encoded_prompt_test = gpt_tokenizer.encode(prompt_text, \n",
        "                                      add_special_tokens=True)\n",
        "\n",
        "encoded_prompt_test"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1026, 373, 43079, 618, 314, 2067, 6155]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ezd0m5c-MrG9",
        "outputId": "694cab5b-ed4a-448a-9a2b-f53ff59b724f"
      },
      "source": [
        "len(encoded_prompt)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JETiy5puMnSF",
        "outputId": "95d942d5-5aeb-44e1-9584-ccfa0565468d"
      },
      "source": [
        "max_length = 50 + len(encoded_prompt)\n",
        "max_length"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U87qInL4MSjL",
        "outputId": "77b5dbca-d4b4-4619-b985-9ece24bdab4a"
      },
      "source": [
        "  output_sequences = gpt_model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=max_length + len (encoded_prompt),\n",
        "      temperature=1.0,\n",
        "      top_k=0,\n",
        "      top_p=0.9,\n",
        "      repetition_penalty=1.2,\n",
        "      do_sample=True,\n",
        "      num_return_sequences=1\n",
        "  )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "193udJisNDnV",
        "outputId": "e7e8e132-17fd-4c3b-eebb-5c7333224e29"
      },
      "source": [
        "len(output_sequences.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUkO3bbNNhsi",
        "outputId": "01a77867-b0c7-44a1-9f49-3c7fd2e2a044"
      },
      "source": [
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "  print(type(generated_sequence)) # torch tensor type\n",
        "  print(generated_sequence) # tensor of numbers/indices\n",
        "  generated_sequence = generated_sequence.tolist()\n",
        "  text = gpt_tokenizer.decode(generated_sequence)\n",
        "  print(text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "tensor([ 1026,   373, 43079,   618,   314,  2067,  6155,    11,   262,  2344,\n",
            "         1107,   257, 36212,    13,  2893,   345,  1394,   510,   351,  3404,\n",
            "          588,   428,   345,  1244,  2051,  1243,    11,   366,   339,  1297,\n",
            "          502,   355,   356, 23667,   319,   262,  1218,  1621,   286,   257,\n",
            "         2156,   287,   262, 24685,   626,    64,   338, 19651,  7404,   286,\n",
            "         2544,  5488])\n",
            "It was raining when I started walking, the wind really a nuisance. While you keep up with stuff like this you might miss things, \" he told me as we descended on the second story of a house in the favela's neighboring village of Davao\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA2e-IyNNTfZ",
        "outputId": "61c2bd23-fc08-4fa4-f14e-3647c96ec2ac"
      },
      "source": [
        "  # What happens when I do not specify num seq\n",
        "  output_sequences = gpt_model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=max_length + len (encoded_prompt),\n",
        "      temperature=10.0, # CHANGED\n",
        "      top_k=0,\n",
        "      top_p=0.9,\n",
        "      do_sample=True\n",
        "  ) # REMOVED repetition_penalty\n",
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    print(type(generated_sequence)) # torch tensor type\n",
        "    print(generated_sequence) # tensor of numbers/indices\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "    text = gpt_tokenizer.decode(generated_sequence)\n",
        "    print(text)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "tensor([ 1026,   373, 43079,   618,   314,  2067,  6155, 21376,  7261, 34848,\n",
            "         4888, 23290, 42786, 13378, 20150, 24255, 27854,  3061, 50137, 28635,\n",
            "          775, 21401, 14609, 47354,  2261, 33440,  2898,  6858,  9070,  3542,\n",
            "        32495, 11126, 39508, 32858, 15178,  4933, 43684, 33510, 44831,  9693,\n",
            "        27008, 32736, 41273, 28582, 46253, 35129, 17949,  7546, 34831, 37532,\n",
            "        49100,  5556])\n",
            "It was raining when I started walking slammed tech Rebellion shared hurry Stev devoted metadatadriving Coul goal DiverCola We Assassin Zombie cellulule impatientTr Andrew Moscow closelsius dismissed Snapdragon rods ScoreUp settles combosEffectiveSH Rochester uninsured dispersemur Miscellaneous jackets anytime approvalKelly Ae creeps plus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4fiJ6oTNgLs",
        "outputId": "d576a30d-6fba-4626-e26d-06e6b6fcff75"
      },
      "source": [
        "  # What happens when I do not specify num seq\n",
        "  output_sequences = gpt_model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=max_length + len (encoded_prompt),\n",
        "      temperature=2.0, # CHANGED\n",
        "      top_k=0,\n",
        "      top_p=0.9,\n",
        "      do_sample=True\n",
        "  )  # REMOVED repetition_penalty\n",
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    print(type(generated_sequence)) # torch tensor type\n",
        "    print(generated_sequence) # tensor of numbers/indices\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "    text = gpt_tokenizer.decode(generated_sequence)\n",
        "    print(text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "tensor([ 1026,   373, 43079,   618,   314,  2067,  6155,   553, 48503, 22960,\n",
            "        40986, 42024, 18998, 13126,  4827, 40068, 33179,  4932, 19362,  3140,\n",
            "        48568,   278, 25237, 15557,   981,  9505, 14525, 29776, 33936, 50176,\n",
            "        14461,  1163, 32820,  6961,  6228,  5096, 14011, 15454,   406, 26183,\n",
            "          198, 37972,   318, 19280,  3833,   319,  1613,  4072,  4245,  7007,\n",
            "         3840, 21742])\n",
            "It was raining when I started walking,\" Baird resembles unab extrateralking golf labor trenches porch Guard Indianapolis ensupering ambient minorities while covering exchanges neatly arresting swath marginal officiates proposal flat insurance nonsfounder Lorman\n",
            " skeptics is blowing pressure on past emb burn requests reasons showcase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVF21wFdMOcd",
        "outputId": "4dd94141-90da-4549-ab79-2bd8615718e4"
      },
      "source": [
        "  # What happens when I do not specify num seq\n",
        "  output_sequences = gpt_model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=max_length + len (encoded_prompt),\n",
        "      temperature=1.0, \n",
        "      top_k=0,\n",
        "      top_p=0.9,\n",
        "      do_sample=True\n",
        "  )  # REMOVED repetition_penalty \n",
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    print(type(generated_sequence)) # torch tensor type\n",
        "    print(generated_sequence) # tensor of numbers/indices\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "    text = gpt_tokenizer.decode(generated_sequence)\n",
        "    print(text)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "tensor([ 1026,   373, 43079,   618,   314,  2067,  6155,  3371,   262,  3770,\n",
            "        17435,    13,   383,  1660,  4966,  6364,   355,   314, 12606,   625,\n",
            "           13,   383,  7150,   477,  1088,   502,   925,   502,  1254,   588,\n",
            "          314,   373,  6476,   281, 22007,   355,   262,  7150,  2405,  7042,\n",
            "          257, 13054,  1022,   514,   290,   262,  8222,    13,  1119,   750,\n",
            "          787,   502])\n",
            "It was raining when I started walking towards the prison gates. The water ran slowly as I crossed over. The trees all around me made me feel like I was facing an obstacle as the trees themselves formed a barrier between us and the forest. They did make me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFxp3vtPLRLQ",
        "outputId": "2f20a30b-1464-43d5-cde8-1ca8064f2f8b"
      },
      "source": [
        "  # What happens when I do not specify num seq\n",
        "  output_sequences = gpt_model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=max_length + len (encoded_prompt),\n",
        "      temperature=1.0,\n",
        "      top_k=1, # CHANGED\n",
        "      top_p=0.9,\n",
        "      do_sample=True\n",
        "  ) \n",
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    print(type(generated_sequence)) # torch tensor type\n",
        "    print(generated_sequence) # tensor of numbers/indices\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "    text = gpt_tokenizer.decode(generated_sequence)\n",
        "    print(text)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "tensor([ 1026,   373, 43079,   618,   314,  2067,  6155,    13,   314,   373,\n",
            "         6155,   287,   262,  6290,    11,   290,   314,   373,  6155,   287,\n",
            "          262,  6290,    11,   290,   314,   373,  6155,   287,   262,  6290,\n",
            "           11,   290,   314,   373,  6155,   287,   262,  6290,    11,   290,\n",
            "          314,   373,  6155,   287,   262,  6290,    11,   290,   314,   373,\n",
            "         6155,   287])\n",
            "It was raining when I started walking. I was walking in the rain, and I was walking in the rain, and I was walking in the rain, and I was walking in the rain, and I was walking in the rain, and I was walking in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8hF9VAxePmn",
        "outputId": "199413f7-cb66-4b58-f955-fb40ee930855"
      },
      "source": [
        "  # What happens when I do not specify num seq\n",
        "  output_sequences = gpt_model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=max_length + len (encoded_prompt),\n",
        "      temperature=1.0,\n",
        "      top_k=10, # ChANGEd\n",
        "      top_p=0.9,\n",
        "      do_sample=True\n",
        "  )\n",
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    print(type(generated_sequence)) # torch tensor type\n",
        "    print(generated_sequence) # tensor of numbers/indices\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "    text = gpt_tokenizer.decode(generated_sequence)\n",
        "    print(text)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "tensor([ 1026,   373, 43079,   618,   314,  2067,  6155,   503,   290,   314,\n",
            "         1422,   470,   892,   881,   546,   340,  1566,   314,   373, 19487,\n",
            "          612,   290,   262,  6766,   373,   655,   523,  4950,    13,   314,\n",
            "         2936,   523, 18259,   284,   307,   503,   612,   290,   284,   423,\n",
            "          326,  1998,    11,   523, 14066,   284,   307,  1498,   284,   466,\n",
            "          644,   314])\n",
            "It was raining when I started walking out and I didn't think much about it until I was halfway there and the sky was just so beautiful. I felt so blessed to be out there and to have that experience, so grateful to be able to do what I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVUW1Vq9LRI1",
        "outputId": "c34f8c7e-70b7-4ec5-d87f-8ada006e3959"
      },
      "source": [
        "  # What happens when I do not specify num seq\n",
        "  output_sequences = gpt_model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=max_length + len (encoded_prompt),\n",
        "      temperature=1.0,\n",
        "      top_k=50, # CHANGED\n",
        "      top_p=0.7, # CHANGED\n",
        "      repetition_penalty=1.2,\n",
        "      do_sample=True\n",
        "  )\n",
        "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    print(type(generated_sequence)) # torch tensor type\n",
        "    print(generated_sequence) # tensor of numbers/indices\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "    text = gpt_tokenizer.decode(generated_sequence)\n",
        "    print(text)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "tensor([ 1026,   373, 43079,   618,   314,  2067,  6155,    11,   290,   340,\n",
            "         1392,  4785,   355,   262,  1110,  1816,   319,    13,   383,  6290,\n",
            "          373, 23147,   866,   422,  2029,   523,   314,   714,  3285,   257,\n",
            "         1256,   286, 14788,  2406,   422,  2354,   616,  2156,   553,   531,\n",
            "          509,  1000,   268,   337,   397,   622,    74,    13,   198,   198,\n",
            "            1,  1858])\n",
            "It was raining when I started walking, and it got worse as the day went on. The rain was pouring down from above so I could hear a lot of screaming coming from outside my house,\" said Kaleen Mabruk.\n",
            "\n",
            "\"There\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ovGs4ge6EP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}