{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde2e3da",
   "metadata": {},
   "source": [
    "# Mask Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cf9aa8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "40a53b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BART_MODEL_NAME = \"facebook/bart-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7e12c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(BART_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "56b064b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(BART_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9108f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_english_phrase = \"UN Chief Says There Is No <mask> in Syria\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1f946e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(example_english_phrase, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6bf7fee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  4154,  1231, 15674,   345,  1534,   440, 50264,    11,  1854,\n",
       "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "703d73b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "45316d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  4154,  2118, 14587, 14704,     2]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d669aa03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNALSO SEE']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ec5064ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>UNALSO SEE</s>']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "22b06761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>UNALSO SEE</s>']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "084a9fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Sample Code from https://huggingface.co/transformers/model_doc/bart.html\n",
    "# from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "# model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", force_bos_token_to_be_generated=True)\n",
    "# tok = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# example_english_phrase = \"UN Chief Says There Is No <mask> in Syria\"\n",
    "# batch = tok(example_english_phrase, return_tensors='pt')\n",
    "# generated_ids = model.generate(batch['input_ids'])\n",
    "# assert tok.batch_decode(generated_ids, skip_special_tokens=True) == ['UN Chief Says There Is No Plan to Stop Chemical Weapons in Syria']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cf05b0",
   "metadata": {},
   "source": [
    "# BART Conditional Generation - Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3279acb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "64a46bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9828fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "67781b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ef2c24ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['My friends']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENTH = 5\n",
    "summary_ids = model.generate(inputs['input_ids'],\n",
    "                             num_beams=4,\n",
    "                             max_length=MAX_LENTH,   # max number of words\n",
    "                             early_stopping=True\n",
    "                            )\n",
    "print(len(summary_ids[0]))\n",
    "print([\n",
    "    tokenizer.decode(g,\n",
    "                     skip_special_tokens=True,\n",
    "                     clean_up_tokenization_spaces=False) for g in summary_ids\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a3667f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['My friends are cool but they eat']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENTH = 10\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=MAX_LENTH, early_stopping=True)\n",
    "print(len(summary_ids[0]))\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fef6c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[\"My friends are cool but they eat too many carbs. I'm not a\"]\n"
     ]
    }
   ],
   "source": [
    "MAX_LENTH = 20\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=MAX_LENTH, early_stopping=True)\n",
    "print(len(summary_ids[0]))\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5447d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"My friends are cool but they eat too many carbs. I'm not a big fan of carbs, but they're good for me. I like to eat a lot of carbs. They're not good for my health. I\"]\n"
     ]
    }
   ],
   "source": [
    "MAX_LENTH = 50\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=MAX_LENTH, early_stopping=True)\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "da13065f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dc52cddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summary_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "14c2d221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[\"My friends are cool but they eat too many carbs. I'm not a big fan of carbs, but they're good for me. I like to eat a lot of carbs. They're not good for my health. I\"]\n"
     ]
    }
   ],
   "source": [
    "MAX_LENTH = 50\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=MAX_LENTH, early_stopping=True)\n",
    "print(len(summary_ids[0]))\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e4b31fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "['public class int get_my_id: public class int gets_my-id: int my_id.public class get_ my-id : int myId: int get my- id. public class get my']\n"
     ]
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = \"public class int get_my_id:\"\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "MAX_LENTH = 50\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=MAX_LENTH, early_stopping=True)\n",
    "print(len(summary_ids[0]))\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7fa01650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "['public class int get_my_id: public class int gets_my-id: int my_id.public class get_ my-id : int myId: int get my- id. public class get my']\n"
     ]
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = \"public class int get_my_id:\"\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "MAX_LENTH = 50\n",
    "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=MAX_LENTH, early_stopping=True)\n",
    "print(len(summary_ids[0]))\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0a1f706e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "50\n",
      "['The fake news dataset is one of the classic text analytics datasets available on Kaggle. It consists of genuine and fake articles’ titles and text from different authors. I have walked through the entire text classification process using traditional machine learning']\n"
     ]
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = '''The fake news dataset is one of the classic text analytics datasets available on Kaggle. \n",
    "It consists of genuine and fake articles’ titles and text from different authors. In this article, \n",
    "I have walked through the entire text classification process using traditional machine learning approaches as well \n",
    "as deep learning.'''\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "print(len(inputs['input_ids'][0]))\n",
    "MIN_LENTH = 20\n",
    "MAX_LENTH = 50\n",
    "summary_ids = model.generate(inputs['input_ids'], min_length=10, max_length=MAX_LENTH, early_stopping=True)\n",
    "print(len(summary_ids[0]))\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "69e15ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "40\n",
      "['The fake news dataset is one of the classic text analytics datasets available on Kaggle. It consists of genuine and fake articles’ titles and text from different authors. In this article']\n"
     ]
    }
   ],
   "source": [
    "ARTICLE_TO_SUMMARIZE = '''The fake news dataset is one of the classic text analytics datasets available on Kaggle. \n",
    "It consists of genuine and fake articles’ titles and text from different authors. In this article, \n",
    "I have walked through the entire text classification process using traditional machine learning approaches as well \n",
    "as deep learning.'''\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
    "print(len(inputs['input_ids'][0]))\n",
    "MIN_LENTH = 20\n",
    "MAX_LENTH = 40\n",
    "summary_ids = model.generate(inputs['input_ids'], min_length=10, max_length=MAX_LENTH, early_stopping=True)\n",
    "print(len(summary_ids[0]))\n",
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e8dd02",
   "metadata": {},
   "source": [
    "## Performance comparison of the NLP Summarization pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "291aae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roynab\\Anaconda3\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "C:\\Users\\roynab\\Anaconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:89: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa767eb4264241e4a9d781fd4d1625c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14b95f3d81f42a7ae1c75bb829786c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac6385202f84cc8a72dcf97b1bdf8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8b92b434b84f819067ea90db46a8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267c1eb612054ea59c5991dd1d4a39eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Fake news dataset is one of the classic text analytics datasets available on Kaggle . It consists of genuine and fake articles’ titles and text from different authors . In this article,'}]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(ARTICLE_TO_SUMMARIZE, max_length=MAX_LENTH, min_length=MIN_LENTH, do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708257ff",
   "metadata": {},
   "source": [
    "# Mask Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9a6a7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "TXT = \"My friends are <mask> but they eat too many carbs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fdc99b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19566b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer([TXT], return_tensors='pt')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "54609a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  2387,   964,    32, 50264,    53,    51,  3529,   350,   171,\n",
       "         33237,     4,     2]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4e2eb90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "52fccbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   100,   524, 50264,    53,    38,  3529,   350,   171, 33237,\n",
       "             4,     2]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TXT = \"I am <mask> but I eat too many carbs.\"\n",
    "input_ids = tokenizer([TXT], return_tensors='pt')['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4589c0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   100,   524, 50264,    53,    38,   524,    67,    10, 27644,\n",
       "             4,     2]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TXT = \"I am <mask> but I am also a believer.\"\n",
    "input_ids = tokenizer([TXT], return_tensors='pt')['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b3a0153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(input_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da4a230f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 14.8692,  -0.7673,  12.1319,  ...,  -2.3899,  -2.6394,   6.6846],\n",
       "         [ 14.8692,  -0.7673,  12.1319,  ...,  -2.3899,  -2.6394,   6.6846],\n",
       "         [-26.5103,  -4.0979,  -0.5074,  ...,  -2.7472,  -3.3270,  -5.5256],\n",
       "         ...,\n",
       "         [-21.8407,  -3.3104,   7.6325,  ...,  -0.9094,  -0.9088,  -1.6164],\n",
       "         [-29.7491,  -3.8424,  11.8108,  ...,  -1.3566,  -1.9058,   6.4403],\n",
       "         [ -6.0542,  -4.1361,  19.9523,  ...,  -6.1178,  -5.3846,   0.9493]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b003f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9bf76601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c9006799",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = logits[0, masked_index].softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7fc92646",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, predictions = probs.topk(5) # top 5 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4cec1434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'an', 'not', 'Catholic', 'also']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(predictions).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac24dcb",
   "metadata": {},
   "source": [
    "# QA with BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "23a626e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForQuestionAnswering\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "92428932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForQuestionAnswering.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1bffe4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "15c0fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "25b7853a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 12375,    21,  2488,   289, 13919,   116,     2,     2, 24021,\n",
       "           289, 13919,    21,    10,  2579, 29771,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7708dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_positions = torch.tensor([1])\n",
    "end_positions = torch.tensor([3])\n",
    "outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
    "loss = outputs.loss\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd763c1",
   "metadata": {},
   "source": [
    "# Experimenting with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2fad902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "java_code = '''public CombinedToken(Integer token, Integer extraToken) {\n",
    "        this.token = token;\n",
    "        this.extraToken = extraToken;\n",
    "    }\n",
    "\n",
    "    public CombinedToken(String data) {\n",
    "        this.tokenString = data;\n",
    "    }'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "44838bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "56364b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([java_code], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "38b2621e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e32d8425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 15110, 23355, 45643,  1640, 49740, 19233,     6, 47927,  1823,\n",
       "         45643,    43, 25522, 50118,  1437,  1437,  1437,  1437,  1437,  1437,\n",
       "          1437,    42,     4, 46657,  5457, 19233,   131, 50118,  1437,  1437,\n",
       "          1437,  1437,  1437,  1437,  1437,    42,     4, 30842, 45643,  5457,\n",
       "          1823, 45643,   131, 50118,  1437,  1437,  1437, 35524, 50140,  1437,\n",
       "          1437,  1437,   285, 23355, 45643,  1640, 34222,   414,    43, 25522,\n",
       "         50118,  1437,  1437,  1437,  1437,  1437,  1437,  1437,    42,     4,\n",
       "         46657, 34222,  5457,   414,   131, 50118,  1437,  1437,  1437, 35524,\n",
       "             2]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8964101d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['public CombinedToken(Integer token, Integer extraToken) {\\n        this.token = token;\\n        this.extraToken = extraToken;\\n    }\\n\\n    public CombinedToken(String data) {\\n        this.tokenString = data;\\n    }']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in inputs['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5a49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
